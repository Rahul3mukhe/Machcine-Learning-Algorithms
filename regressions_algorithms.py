# -*- coding: utf-8 -*-
"""Regressions_algorithms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RpfHeR6shXEvw2nc8e1jGQULgs9RpAeG
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()

# Access data and target
X = housing.data
y = housing.target

import pandas as pd

# Convert to DataFrame for easier analysis
df = pd.DataFrame(housing.data, columns=housing.feature_names)
df['target'] = housing.target

# Display basic information and statistics
print(df.info())
print(df.describe())

df

dataset = pd.DataFrame(df.values, columns=df.columns)

dataset

dataset.head()

#independent features and dependent features
x= dataset
y= df.target

y

#train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    x, y, test_size = 0.30, random_state = 42)

#standardising the dataset
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)

X_test = scaler.transform(X_test)

#implementing Linear regression
from sklearn.linear_model import LinearRegression
#ceoss validation
from sklearn.model_selection import cross_val_score

regression = LinearRegression()
regression.fit(X_train,y_train)

mse = cross_val_score(regression, X_train, y_train, scoring = 'neg_mean_squared_error', cv = 5)

np.mean(mse)

#prediction
reg_pred = regression.predict(X_test)

reg_pred

import seaborn as sns
sns.distplot(reg_pred - y_test)

from sklearn.metrics import r2_score
score = r2_score(y_test, reg_pred)

score

"""# **RIDGE REGRESSION**"""

from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

ridge_regressor = Ridge()

ridge_regressor

parameters = {'alpha' : [1,2,4,65,76,34,75]}
ridgecv = GridSearchCV(ridge_regressor, parameters, scoring = 'neg_mean_squared_error', cv = 5)
ridgecv.fit(X_train, y_train)

print(ridgecv.best_params_)

print(ridgecv.best_score_)

ridge_pred = ridgecv.predict(X_test)

sns.kdeplot(ridge_pred - y_test)
plt.title('KDE Plot of Prediction Error')
plt.xlabel('Prediction Error')
plt.ylabel('Density')
plt.show()

score = r2_score(y_test, ridge_pred)

score

"""# **LASSO REGRESSION**"""

from sklearn.linear_model import Lasso

lasso = Lasso()

parameters = {'alpha' : [1,2,4,65,76,34,75]}
lassocv = GridSearchCV(lasso, parameters, scoring = 'neg_mean_squared_error', cv = 5)
lassocv.fit(X_train, y_train)

print(lassocv.best_params_)
print(lassocv.best_score_)

lasso_pred = lassocv.predict(X_test)

sns.kdeplot(lasso_pred - y_test)
plt.title('KDE Plot of Prediction Error')
plt.xlabel('Prediction Error')
plt.ylabel('Density')
plt.show()

